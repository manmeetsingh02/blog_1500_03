<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Manmeet Singh - Emerging Technology Blog</title>
    <link href="css/style.css" rel="stylesheet" type="text/css" />
    </head>
<body>
    <h1>Emerging Technologies</h1>
    <main>
        <!-- sample of a blog post -->
        <article>
            <h2>Artifical Intelligence</h2>
            <img src="../blog_1500_03/images/human-like-robot-and-artificial-intelligence-2022-01-06-00-25-53-utc.jpg" alt="Artifical Intelligence Technology">
            <pre><a href="https://www.accenture.com/ca-en/insights/artificial-intelligence-summary-index?c=acn_glb_brandexpressiongoogle_12362533&n=psgs_0921&gclid=Cj0KCQiAgribBhDkARIsAASA5bvmosXMrPI0m_Ex0f5sYs0T63Aijl1SmAhc375euIr9VR1cSeRAa68aAjAKEALw_wcB&gclsrc=aw.ds">Here is Artificial Intelligence</a>
          </pre>
           <time>November 11, 2022</time>
            <p>Artificial intelligence is a constellation of many different technologies working together to enable machines to sense, comprehend, act, and learn with human-like levels of intelligence. Maybe that’s why it seems as though everyone’s definition of artificial intelligence is different: AI isn’t just one thing.</p>
            <p>Technologies like machine learning and natural language processing are all part of the AI landscape. Each one is evolving along its own path and, when applied in combination with data, analytics and automation, can help businesses achieve their goals, be it improving customer service or optimizing the supply chain. Artificial intelligence is a constellation of many different technologies working together to enable machines to sense, comprehend, act, and learn with human-like levels of intelligence. Maybe that’s why it seems as though everyone’s definition of artificial intelligence is different: AI isn’t just one thing.

                Technologies like machine learning and natural language processing are all part of the AI landscape. Each one is evolving along its own path and, when applied in combination with data, analytics and automation, can help businesses achieve their goals, be it improving customer service or optimizing the supply chain.
                
                Narrow (or “weak”) AI
                Some go even further to define artificial intelligence as “narrow” and “general” AI. Most of what we experience in our day-to-day lives is narrow AI, which performs a single task or a set of closely related tasks. Examples include:</p>
                
                <p>Weather apps
                Digital assistants
                Software that analyzes data to optimize a given business function
                These systems are powerful, but the playing field is narrow: They tend to be focused on driving efficiencies. But, with the right application, narrow AI has immense transformational power—and it continues to influence how we work and live on a global scale.
                
                General (or “strong”) AI
                General AI is more like what you see in sci-fi films, where sentient machines emulate human intelligence, thinking strategically, abstractly and creatively, with the ability to handle a range of complex tasks. While machines can perform some tasks better than humans (e.g. data processing), this fully realized vision of general AI does not yet exist outside the silver screen. That’s why human-machine collaboration is crucial—in today’s world, artificial intelligence remains an extension of human capabilities, not a replacement.</p>
      <p>There are many ways to define artificial intelligence, but the more important conversation revolves around what AI enables you to do.

        End-to-end efficiency: AI eliminates friction and improves analytics and resource utilization across your organization, resulting in significant cost reductions. It can also automate complex processes and minimize downtime by predicting maintenance needs.
        
        Improved accuracy and decision-making: AI augments human intelligence with rich analytics and pattern prediction capabilities to improve the quality, effectiveness, and creativity of employee decisions.
        
        Intelligent offerings: Because machines think differently from humans, they can uncover gaps and opportunities in the market more quickly, helping you introduce new products, services, channels and business models with a level of speed and quality that wasn’t possible before.
        
        Empowered employees: AI can tackle mundane activities while employees spend time on more fulfilling high-value tasks. By fundamentally changing the way work is done and reinforcing the role of people to drive growth, AI is projected to boost labor productivity. Using AI can also unlock the incredible potential of talent with disabilities, while helping all workers thrive.
        
        Superior customer service: Continuous machine learning provides a steady flow of 360-degree customer insights for hyper personalization. From 24/7 chatbots to faster help desk routing, businesses can use AI to curate information in real time and provide high-touch experiences that drive growth, retention and overall satisfaction.
        
        AI is used in many ways, but the prevailing truth is that your AI strategy is your business strategy. To maximize your return on AI investments, identify your business priorities and then determine how AI can help.
        
        Identify your business priorities and then determine how AI can help.</p>
    
      <p>    The future of AI
        According to Accenture’s report, AI: Built to Scale, 84 percent of business executives believe they need to use AI to achieve their growth objectives. However, 76 percent acknowledge struggling with how to scale AI across their business. Until now, there hasn’t been a blueprint to getting past proof of concept into production and scale, a transition many struggle to make. At this inflection point, it’s imperative businesses take the necessary steps to scale successfully.
        
        84%
        of business executives believe they need to use AI to achieve their growth objectives.
        
        76%
        acknowledge struggling with how to scale AI across their business.
        
        Define your business value
        There are countless ways to use AI. How do organizations decide what to focus on? To scale successfully, start by defining what value means to your business. Then assess and prioritize the various applications of AI against those strategic objectives.
        
        Rework your workforce
        The growing momentum of AI calls for a diverse, reconfigured workforce to support and scale it. Despite early fears that artificial intelligence and automation would lead to job loss, the future of AI hinges on human-machine collaboration and the imperative to reshape talent and ways of working.
        
        Establish governance and ethical frameworks
        Organizations must design their AI strategy with trust in mind. That means building the right governance structures and making sure ethical principles are translated into the development of algorithms and software.
        
        Applying these factors successfully can help organizations unlock exponential value and stay competitive. AI is no longer simply a "nice to have", but is critical to a business’ future.
        
        </p>
        <p>AI ethics
        No artificial intelligence introduction would be complete without addressing AI ethics. AI is moving at a blistering pace and, as with any powerful technology, organizations need to build trust with the public and be accountable to their customers and employees.
        
        At Accenture, we define “responsible AI” as the practice of designing, building and deploying AI in a manner that empowers employees and businesses and fairly impacts customers and society—allowing companies to engender trust and scale AI with confidence.
        
        Trust
        Every company using AI is subject to scrutiny. Ethics theater, where companies amplify their responsible use of AI through PR while partaking in unpublicized gray-area activities, is a regular issue. Unconscious bias is yet another. Responsible AI is an emerging capability aiming to build trust between organizations and both their employees and customers.
        
        Data security
        Data privacy and the unauthorized use of AI can be detrimental both reputationally and systemically. Companies must design confidentiality, transparency and security into their AI programs at the outset and make sure data is collected, used, managed and stored safely and responsibly.
        
        Transparency and explainability
        Whether building an ethics committee or revising their code of ethics, companies need to establish a governance framework to guide their investments and avoid ethical, legal and regulatory risks. As AI technologies become increasingly responsible for making decisions, businesses need to be able to see how AI systems arrive at a given outcome, taking these decisions out of the “black box.” A clear governance framework and ethics committee can help with the development of practices and protocols that ensure their code of ethics is properly translated into the development of AI solutions.
        
        Control
        Machines don’t have minds of their own, but they do make mistakes. Organizations should have risk frameworks and contingency plans in place in the event of a problem. Be clear about who is accountable for the decisions made by AI systems, and define the management approach to help escalate problems when necessary.
        
        Organizations need to build trust with the public and be accountable to their.</p>  </article>

        <article>
        <h3>Robotization</h3>
        <img src="../blog_1500_03/images/roboat.jpeg" alt="Robotization: the advantage of developing economies on developed ones.">
        <pre><a href="https://medium.com/urban-ai/robotization-the-advantage-of-developing-economies-on-developed-ones-6106e0443433">Robotization: the advantage of developing economies on developed ones </a>
       </pre> <time>November 15, 2022</time>
        <p>In the specific context of an interview by Oliver Mitchell, VC investor at FF Venture Capital, which is also writing from time to time for the Robot Report. I shared my experience of the digital transformation from my case studies in Taiwan, Israel, and Estonia, with a specific focus on disruptive technologies and the advent of robotization in our cities.

          Explain the genesis of your work, what prompted you to study AI in SmartCities?
          
          I started real estate development in a family business in the South of France, after my graduation in psychology. I’ve been attracted early by the potential of ICT to modernize the way we were producing housing facilities, and to overpass my challengers which kept thinking of this business in a traditional (old-fashioned) vision. Since I had this background studies in social science, I’ve been naturally committed to using the internet as a social innovation tool and started to experience different ways to introduce participatory decision-making with the customers of the company. Step by step, this approach broadened, becoming a will to co-design urban areas with neighbors, and finally citizen engagement in smart-city.
          
          I’ve been refining my collaborative management methodology for 10 years, taking various roles in the stakeholders of a city building: non-profit associations to sound out inhabitants of different districts, a think tank with private business owners to build a future-oriented project for my hometown and influence local politics, the real estate company asked for specific visions for land management and a very grounded business model from the existing housing market, and the startups offered that possibilities to experience new business models, trying to reach the support from investors and fast prototyping digital projects.
          
          Finally, after some years of experience, I joined an academic research laboratory, to strengthen my field expertise with some scientific establishment and data evidence. I also used that opportunity to switch my working language from French to English, grow an international network on my topic, and publish open valuable content and analysis on the web to share my experience and be recognized as a legitimate expert on my field. After having collected data in different case studies, I’ve been learning Python to make my data analysis and was obviously tempted to try some machine learning on my datasets. Once I understood how ML works, I built my own algorithms to analyze citizen engagement in smart cities and started to present it to some fellow researchers, or entrepreneurs of civic technologies and smart cities. The feedback has been very enthusiastic and so I kept pushing my AI models forward by finally coding two computer simulations from my data analysis which allows me to make some more inferences outside of my datasets.
          
          <p> How would you rate the implementations of technology within the urban environment in the three areas you evaluated? Can summarize some good and bad use cases? Have you followed up to see how citizens are engaging with the evolution of their environments? Summarize the regional approaches in Israel, Taiwan, and Estonia, and what are lessons for the USA?
          
          In my case studies, I focused on evaluating the citizen engagement dynamics, both digital and analog, with a predominant observation of the smart devices offered by local governments to advent new electronic citizenship facilities capable to refurbish the contemporary representative democracies in a more direct democracy fashion. My three case studies Taiwan, Israel, and Estonia are some of the most advanced digital democracies in the world, with a high penetration rate of technology. Taiwan is the number one provider of micro-components in the world, making its industry essential to the smartphones’ and computers’ global markets. The population benefits from an internet penetration rate of more than 83% overall. Science and technology are one of Israel’s most developed fields: the state spent the highest ratio in the world of its GDP in civil research and development in 2015. Local companies, students, and citizens benefit from many public and private tech hubs to support innovation. Estonia is famous as the digital nation because they developed the most advanced digital state facilities, which led to dematerialize 99,5% of the public services resulting in about 98% of all banking transactions and public interaction happening online. Interesting to know that the only offline services: buying a flat and getting married or divorced, remain offline by the decision of the government.
          
          Taiwan, Israel, and Estonia are commonly three small countries, recently independent. Their modern economy arose in the ’90s after strategic choices of development in tech, science, and education, due to a lack of other natural resources than the human one to exploit autonomously. The fast transition to democracies sustained by free-market economies was urged by the necessity to support their existence as independent nations on the sidelines of robust less democratic regimes at their borders. In the three cases, the first customer, in crucial need of innovation, was the state: the public sector ordered a deep refurbishment or reconstruction of main infrastructures and public services. This state transformation created many opportunities for local entrepreneurs to develop companies dedicated to filling the demand in innovation, new technology, scientific research, and education. This dynamic has set the basis of a service economy with the best technicality levels and financial support to innovation. Secondly, the government helps companies reach global markets since they tend to communicate together on their achievement.
          
          A big difference with the USA, which is common to those three countries, is that they are too small to self-sustain their markets. If their enterprises want to grow up to crucial stability scales, they need to look for customers abroad, which tends to force them to shape their businesses in the way to satisfy emerging markets, while the USA as a highly advanced and mature market can already scale nationally and grow successful unicorns and millionaires inside its own borders. As a result, the USA sometimes appears abroad as a dominant force willing to take over emerging markets to feed its own interests instead of a cooperating partner.
          
        </p> 
        <p> Based upon your findings what is the right balance for implementing disruptive technologies(e.g., autonomous vehicles, commercial drones, remote sensing/vision, data analytics, etc) for city planners to engage community stakeholders? How does this determine the adoption rate and success of such early trials/deployments? What are the lessons learned from high-profile failures?
          
          The implementation of disruptive technologies is a tricky move since it needs collaboration of most stakeholders of the society, or at least a combination of elements while it usually implies a loss for existing actors who can’t adapt to the change. What made the success of Taiwan, Israel, and Estonia is to have invested in human resources and education to build their contemporary societies. As a result, a wide part of the population is aware of science and technology, works on research and innovation both into the academic and the private sectors, and the population is conscious that it needs to keep evolving and implementing new technologies to support their ideal position in the globalization phenomena. There is a collective intelligence over there that knows that they have more to win in accepting disruptive changes instead of sleeping on its past achievements. This is a mindset that is lacking in the oldest free-market economies and democracies nowadays, like the USA, Europe, or Japan.
          
          This mindset does not mean that there is less failure in experimentations and early adoptions. The process of innovation and startups is the same everywhere and requires a lot of trials and errors before it gives satisfying results. For that reason, it can’t be otherwise than a collective process where stakeholders of the same ecosystem share the risks and the benefits together. Being a small national people in terms of demography, and having the consciousness of sharing a common destiny, helps in creating cohesion among the population, and so being naturally committed to its national success.
          
          </p>
          
          <p> What is your advice to entrepreneurs, especially roboticists, in tackling the problems of cities with unmanned systems?
          
          All kinds of automation are great, BUT a city is about satisfying the quality of life of its residents before everything. Integrated dashboards with AI and robots, and tech-supported decision makings are a life-long technical dream of all generations of urban professionals, engineers, architects, but most people would agree that there would not be worse governance than the one of a scientist’s government. For the simple reason that humans are sensitive entities more than rational ones, highly unpredictable and so far never completely understood by any science.
          
          My first advice would be to keep humble in its ambitions to change human peers. Our generation already benefits daily from the use of high-speed calculation infrastructures which are our smartphones, our computers, and all the digital tech behind us. Pervasive computing is promising unprecedented advances in human/environment interactions and the progress in the science of interfaces and robotization is looking for ways to make our lives more comfortable, lower our consumption of resources, and build more sustainable cities and lifestyles. Somehow this is just an extension of the famous laws of robotics published by Asimov in 1950: robots must be programmed to support human life, and not the contrary.</p>
        </article>

<article>
          <h4>Virtual Reality</h4>
          <img src="../blog_1500_03/images/virtual reality.jpeg" alt="virtual reality
          computer science">
          <pre><a href="https://www.britannica.com/technology/virtual-reality/Education-and-training">virtual reality computer science</a>
        </pre>
         <time>November 21, 2022</time>
         <p>virtual reality (VR), the use of computer modeling and simulation that enables a person to interact with an artificial three-dimensional (3-D) visual or other sensory environment. VR applications immerse the user in a computer-generated environment that simulates reality through the use of interactive devices, which send and receive information and are worn as goggles, headsets, gloves, or body suits. In a typical VR format, a user wearing a helmet with a stereoscopic screen views animated images of a simulated environment. The illusion of “being there” (telepresence) is effected by motion sensors that pick up the user’s movements and adjust the view on the screen accordingly, usually in real time (the instant the user’s movement takes place). Thus, a user can tour a simulated suite of rooms, experiencing changing viewpoints and perspectives that are convincingly related to his own head turnings and steps. Wearing data gloves equipped with force-feedback devices that provide the sensation of touch, the user can even pick up and manipulate objects that he sees in the virtual environment.

          The term virtual reality was coined in 1987 by Jaron Lanier, whose research and engineering contributed a number of products to the nascent VR industry. A common thread linking early VR research and technology development in the United States was the role of the federal government, particularly the Department of Defense, the National Science Foundation, and the National Aeronautics and Space Administration (NASA). Projects funded by these agencies and pursued at university-based research laboratories yielded an extensive pool of talented personnel in fields such as computer graphics, simulation, and networked environments and established links between academic, military, and commercial work. The history of this technological development, and the social context in which it took place, is the subject of this article.</p>
          <p>Early work
            Artists, performers, and entertainers have always been interested in techniques for creating imaginative worlds, setting narratives in fictional spaces, and deceiving the senses. Numerous precedents for the suspension of disbelief in an artificial world in artistic and entertainment media preceded virtual reality. Illusionary spaces created by paintings or views have been constructed for residences and public spaces since antiquity, culminating in the monumental panoramas of the 18th and 19th centuries. Panoramas blurred the visual boundaries between the two-dimensional images displaying the main scenes and the three-dimensional spaces from which these were viewed, creating an illusion of immersion in the events depicted. This image tradition stimulated the creation of a series of media—from futuristic theatre designs, stereopticons, and 3-D movies to IMAX movie theatres—over the course of the 20th century to achieve similar effects. For example, the Cinerama widescreen film format, originally called Vitarama when invented for the 1939 New York World’s Fair by Fred Waller and Ralph Walker, originated in Waller’s studies of vision and depth perception. Waller’s work led him to focus on the importance of peripheral vision for immersion in an artificial environment, and his goal was to devise a projection technology that could duplicate the entire human field of vision. The Vitarama process used multiple cameras and projectors and an arc-shaped screen to create the illusion of immersion in the space perceived by a viewer. Though Vitarama was not a commercial hit until the mid-1950s (as Cinerama), the Army Air Corps successfully used the system during World War II for anti-aircraft training under the name Waller Flexible Gunnery Trainer—an example of the link between entertainment technology and military simulation that would later advance the development of virtual reality.
            
            Sensory stimulation was a promising method for creating virtual environments before the use of computers. After the release of a promotional film called This Is Cinerama (1952), the cinematographer Morton Heilig became fascinated with Cinerama and 3-D movies. Like Waller, he studied human sensory signals and illusions, hoping to realize a “cinema of the future.” By late 1960, Heilig had built an individual console with a variety of inputs—stereoscopic images, motion chair, audio, temperature changes, odours, and blown air—that he patented in 1962 as the Sensorama Simulator, designed to “stimulate the senses of an individual to simulate an actual experience realistically.” During the work on Sensorama, he also designed the Telesphere Mask, a head-mounted “stereoscopic 3-D TV display” that he patented in 1960. Although Heilig was unsuccessful in his efforts to market Sensorama, in the mid-1960s he extended the idea to a multiviewer theatre concept patented as the Experience Theater and a similar system called Thrillerama for the Walt Disney Company.
            
           <p> 
            White male businessman works a touch screen on a digital tablet. Communication, Computer Monitor, Corporate Business, Digital Display, Liquid-Crystal Display, Touchpad, Wireless Technology, iPad
            BRITANNICA QUIZ
            Gadgets and Technology: Fact or Fiction?
            Is virtual reality only used in toys? Have robots ever been used in battle? From computer keyboards to flash memory, learn about gadgets and technology in this quiz.
            The seeds for virtual reality were planted in several computing fields during the 1950s and ’60s, especially in 3-D interactive computer graphics and vehicle/flight simulation. Beginning in the late 1940s, Project Whirlwind, funded by the U.S. Navy, and its successor project, the SAGE (Semi-Automated Ground Environment) early-warning radar system, funded by the U.S. Air Force, first utilized cathode-ray tube (CRT) displays and input devices such as light pens (originally called “light guns”). By the time the SAGE system became operational in 1957, air force operators were routinely using these devices to display aircraft positions and manipulate related data.
            
            During the 1950s, the popular cultural image of the computer was that of a calculating machine, an automated electronic brain capable of manipulating data at previously unimaginable speeds. The advent of more affordable second-generation (transistor) and third-generation (integrated circuit) computers emancipated the machines from this narrow view, and in doing so it shifted attention to ways in which computing could augment human potential rather than simply substituting for it in specialized domains conducive to number crunching. In 1960 Joseph Licklider, a professor at the Massachusetts Institute of Technology (MIT) specializing in psychoacoustics, posited a “man-computer symbiosis” and applied psychological principles to human-computer interactions and interfaces. He argued that a partnership between computers and the human brain would surpass the capabilities of either alone. As founding director of the new Information Processing Techniques Office (IPTO) of the Defense Advanced Research Projects Agency (DARPA), Licklider was able to fund and encourage projects that aligned with his vision of human-computer interaction while also serving priorities for military systems, such as data visualization and command-and-control systems.
            
            
            Get a Britannica Premium subscription and gain access to exclusive content.
            Subscribe Now
            Another pioneer was electrical engineer and computer scientist Ivan Sutherland, who began his work in computer graphics at MIT’s Lincoln Laboratory (where Whirlwind and SAGE had been developed). In 1963 Sutherland completed Sketchpad, a system for drawing interactively on a CRT display with a light pen and control board. Sutherland paid careful attention to the structure of data representation, which made his system useful for the interactive manipulation of images. In 1964 he was put in charge of IPTO, and from 1968 to 1976 he led the computer graphics program at the University of Utah, one of DARPA’s premier research centres. In 1965 Sutherland outlined the characteristics of what he called the “ultimate display” and speculated on how computer imagery could construct plausible and richly articulated virtual worlds. His notion of such a world began with visual representation and sensory input, but it did not end there; he also called for multiple modes of sensory input. DARPA sponsored work during the 1960s on output and input devices aligned with this vision, such as the Sketchpad III system by Timothy Johnson, which presented 3-D views of objects; Larry Roberts’s Lincoln Wand, a system for drawing in three dimensions; and Douglas Engelbart’s invention of a new input device, the computer mouse.
            
            
            early head-mounted display device
            early head-mounted display device</p>
            <p>Within a few years, Sutherland contributed the technological artifact most often identified with virtual reality, the head-mounted 3-D computer display. In 1967 Bell Helicopter (now part of Textron Inc.) carried out tests in which a helicopter pilot wore a head-mounted display (HMD) that showed video from a servo-controlled infrared camera mounted beneath the helicopter. The camera moved with the pilot’s head, both augmenting his night vision and providing a level of immersion sufficient for the pilot to equate his field of vision with the images from the camera. This kind of system would later be called “augmented reality” because it enhanced a human capacity (vision) in the real world. When Sutherland left DARPA for Harvard University in 1966, he began work on a tethered display for computer images (see photograph). This was an apparatus shaped to fit over the head, with goggles that displayed computer-generated graphical output. Because the display was too heavy to be borne comfortably, it was held in place by a suspension system. Two small CRT displays were mounted in the device, near the wearer’s ears, and mirrors reflected the images to his eyes, creating a stereo 3-D visual environment that could be viewed comfortably at a short distance. The HMD also tracked where the wearer was looking so that correct images would be generated for his field of vision. The viewer’s immersion in the displayed virtual space was intensified by the visual isolation of the HMD, yet other senses were not isolated to the same degree and the wearer could continue to walk around.</p>
<p>
            Education and training
            Learn how virtual training programs using simulators benefit various professions
            Learn how virtual training programs using simulators benefit various professionsSee all videos for this article
            An important area of application for VR systems has always been training for real-life activities. The appeal of simulations is that they can provide training equal or nearly equal to practice with real systems, but at reduced cost and with greater safety. This is particularly the case for military training, and the first significant application of commercial simulators was pilot training during World War II. Flight simulators rely on visual and motion feedback to augment the sensation of flying while seated in a closed mechanical system on the ground. The Link Company, founded by former piano maker Edwin Link, began to construct the first prototype Link Trainers during the late 1920s, eventually settling on the “blue box” design acquired by the Army Air Corps in 1934. The first systems used motion feedback to increase familiarity with flight controls. Pilots trained by sitting in a simulated cockpit, which could be moved hydraulically in response to their actions (see ). Later versions added a “cyclorama” scene painted on a wall outside the simulator to provide limited visual feedback. Not until the Celestial Navigation Trainer, commissioned by the British government in World War II, were projected film strips used in Link Trainers—still, these systems could only project what had been filmed along a correct flight or landing path, not generate new imagery based on a trainee’s actions. By the 1960s, flight trainers were using film and closed-circuit television to enhance the visual experience of flying. The images could be distorted to generate flight paths that diverted slightly from what had been filmed; sometimes multiple cameras were used to provide different perspectives, or movable cameras were mounted over scale models to depict airports for simulated landings.
            
            Inspired by the controls in the Link flight trainer, Sutherland suggested that such displays include multiple sensory outputs, force-feedback joysticks, muscle sensors, and eye trackers; a user would be fully immersed in the displayed environment and fly through “concepts which never before had any visual representation.” In 1968 he moved to the University of Utah, where he and his colleague David Evans founded Evans & Sutherland Computer Corporation. The new company initially focused on the development of graphics applications, such as scene generators for flight simulator systems. These systems could render scenes at roughly 20 frames per second in the early 1970s, about the minimum frame rate for effective flight training. General Electric Company constructed the first flight simulators with built-in, real-time computer image generation, first for the Apollo program in the 1960s, then for the U.S. Navy in 1972. By the mid-1970s, these systems were capable of generating simple 3-D models with a few hundred polygon faces; they utilized raster graphics (collections of dots) and could model solid objects with textures to enhance the sense of realism (see computer graphics). By the late 1970s, military flight simulators were also incorporating head-mounted displays, such as McDonnell Douglas Corporation’s VITAL helmet, primarily because they required much less space than a projected display. A sophisticated head tracker in the HMD followed a pilot’s eye movements to match computer-generated images (CGI) with his view and handling of the flight controls.</p>
       </p> </article>
        <!-- end of sample -->
    </main>
</body>
</html>
